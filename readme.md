<!-- <img src="assets/logo.png"> -->
<h3  align="center"> ConversationTTS: A Speech Foundation Model for Multilingual Conversational Text-to-Speech</h3>


## Introduction

We release the training and inference code for ConversationTTS. Also, we release the first checkpoint, which trained on 1.5 epoch on about 20w hours speech data.

## Usage
### ‚ö° Quick Start  

### üõ†Ô∏è Local Deployment  
Install and Run CapSpeech locally.  
- üíø Installation & Usage: [üìÑ Instrucitons](docs/quick_use.md)

## Development
Please refer to the following documents to prepare the data, train the model, and evaluate its performance.
- [Data Preparation](docs/dataset.md)  
- [Training](docs/training.md)  
- [Evaluation](capspeech/eval/README.md)  

## Main Contributors

- [Dongchao Yang]
- [Dading Cong]
- [Jiankun Zhao]
- [Yuanyuan Wang]

## Citation

If you find this work useful, please consider contributing to this repo and cite this work:


## License
All datasets, listening samples, source code, pretrained checkpoints, and the evaluation toolkit are licensed under the Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0).  
See the [LICENSE](./LICENSE) file for details.

## Acknowledgements

This implementation is based on [UniAudio](https://github.com/yangdongchao/UniAudio), [CSM](https://github.com/SesameAILabs/csm), [Moshi](https://github.com/kyutai-labs/moshi), [RSTNet](https://github.com/yangdongchao/RSTnet). We appreciate their awesome work.

## üåü Like This Project?
If you find this repo helpful or interesting, consider dropping a ‚≠ê ‚Äî it really helps and means a lot!

